{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Taxon_GPT_API ###\n",
    "## Introduction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9a8be3b30a5f8a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 需要使用一个部分来介绍如何正确的获得一个OpenAi公司的API_key，以便于后续对于API的调用使用"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad2c4d9349035822"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 如何在Python环境中调用这个key，将这个API作为环境变量在终端或者powershell使用"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "159ef41b927eeb9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 首先起始的部分是关于使用loads相关的安装包"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6598279122552a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "from openai import OpenAI  # For interacting with OpenAI API\n",
    "import os  # For interacting with the operating system, such as file paths\n",
    "import re  # For regular expressions, useful for pattern matching in strings\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第一个步骤导入相关需要使用到的文件，对于Nexus文件的解析以及转换为各种的文件类型\n",
    "（我可以在这个部分中插入之前制作的部分流程图，来帮助理解）\n",
    "# 导入文件：Nexus文件，prompt文件\n",
    "# 输出文件：csv文件，knowledge_graph的JSON格式文件，character_info文件"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6e1a53b835d9d6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 1\n",
    "\n",
    "# Function to convert a letter to a number based on its position in the alphabet\n",
    "def letter_to_number(letter):\n",
    "    return str(ord(letter) - ord('A') + 10)\n",
    "\n",
    "\n",
    "# Function to parse the matrix content from NEXUS format\n",
    "def parse_matrix(matrix_content):\n",
    "    data = []\n",
    "    headers = []\n",
    "    lines = matrix_content.strip().split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        taxa = lines[i].strip().strip(\"'\")  # Extract taxa name\n",
    "        traits = lines[i + 1].strip()  # Extract traits for the taxa\n",
    "        species_traits = []\n",
    "        j = 0\n",
    "        while j < len(traits):\n",
    "            if traits[j] == '(':  # Handle compound states\n",
    "                j += 1\n",
    "                states = ''\n",
    "                while traits[j] != ')':\n",
    "                    if traits[j].isalpha():\n",
    "                        states += letter_to_number(traits[j])\n",
    "                    else:\n",
    "                        states += traits[j]\n",
    "                    j += 1\n",
    "                species_traits.append(','.join(states))\n",
    "            elif traits[j] == '?':\n",
    "                species_traits.append('Missing')  # Missing data\n",
    "            elif traits[j] == '-':\n",
    "                species_traits.append('Not Applicable')  # Not applicable data\n",
    "            elif traits[j].isalpha():\n",
    "                species_traits.append(letter_to_number(traits[j]))  # Convert letter to number\n",
    "            else:\n",
    "                species_traits.append(traits[j])  # Directly append the trait\n",
    "            j += 1\n",
    "        data.append([taxa] + species_traits)  # Append the parsed traits\n",
    "    max_traits = max(len(row) - 1 for row in data)\n",
    "    headers = ['taxa'] + [f'Character{i + 1}' for i in range(max_traits)]  # Create headers for DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=headers)  # Create DataFrame\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to convert NEXUS file to CSV format\n",
    "def convert_nexus_to_csv(file_path, output_path):\n",
    "    try:\n",
    "        encodings = ['utf-8', 'gbk', 'latin1']  # List of encodings to try\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file with encoding: {encoding}\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to read file with encoding: {encoding}\")\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"Failed to read file with all attempted encodings.\")\n",
    "\n",
    "        # Extract the MATRIX section from the NEXUS file content\n",
    "        matrix_content = re.search(r'MATRIX\\s*(.*?)\\s*;', content, re.DOTALL).group(1).strip()\n",
    "        df = parse_matrix(matrix_content)  # Parse the matrix content into a DataFrame\n",
    "        df.to_csv(output_path, index=False)  # Save DataFrame as CSV\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Function to build a knowledge graph from the parsed matrix\n",
    "def build_knowledge_graph(matrix):\n",
    "    knowledge_graph = {}\n",
    "    for _, row in matrix.iterrows():\n",
    "        taxa = row.iloc[0]\n",
    "        characteristics = {}\n",
    "        for col in matrix.columns[1:]:\n",
    "            state = row[col]\n",
    "            if isinstance(state, str) and ',' in state:\n",
    "                state = state.replace(',', ' and ')\n",
    "            characteristics[col] = str(state)\n",
    "        knowledge_graph[taxa] = {'Characteristics': characteristics}\n",
    "    return knowledge_graph\n",
    "\n",
    "\n",
    "# Function to save the knowledge graph as a JSON file\n",
    "def save_knowledge_graph_as_json(knowledge_graph, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(knowledge_graph, f, indent=4)\n",
    "\n",
    "# Function to convert NEXUS to knowledge graph and save as CSV and JSON\n",
    "def nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path):\n",
    "    # Step 1: Convert NEXUS to CSV\n",
    "    df = convert_nexus_to_csv(nexus_file_path, csv_output_path)\n",
    "\n",
    "    if df is not None:\n",
    "        # Step 2: Build the knowledge graph from the DataFrame\n",
    "        knowledge_graph = build_knowledge_graph(df)\n",
    "\n",
    "        # Step 3: Save the knowledge graph as a JSON file\n",
    "        save_knowledge_graph_as_json(knowledge_graph, json_output_path)\n",
    "\n",
    "        return knowledge_graph\n",
    "    else:\n",
    "        print(\"Failed to create the DataFrame from NEXUS file.\")\n",
    "        return None\n",
    "\n",
    "# Function to parse CHARLABELS section of the NEXUS file\n",
    "def parse_charlabels(charlabels_content):\n",
    "    charlabels = {}\n",
    "    lines = charlabels_content.strip().split(\"\\n\")\n",
    "    char_pattern = re.compile(r\"\\[(\\d+)\\(\\d+\\)\\]\\s+'(.+?)'\")\n",
    "    for line in lines:\n",
    "        match = char_pattern.match(line.strip().rstrip(','))\n",
    "        if match:\n",
    "            char_index = int(match.group(1))\n",
    "            description = match.group(2)\n",
    "            charlabels[char_index] = description\n",
    "    return charlabels\n",
    "\n",
    "# Function to parse STATELABELS section of the NEXUS file\n",
    "def parse_statelabels(statelabels_content):\n",
    "    statelabels = {}\n",
    "    lines = statelabels_content.strip().split(\"\\n\")\n",
    "    current_char = None\n",
    "    states = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r'^\\d+', line):\n",
    "            if current_char is not None:\n",
    "                statelabels[current_char] = states\n",
    "            parts = line.split(' ', 1)\n",
    "            current_char = int(parts[0])\n",
    "            states = parts[1].strip().strip(',').split(\"' '\")\n",
    "            states = [state.strip(\"'\") for state in states]\n",
    "        else:\n",
    "            additional_states = line.strip().strip(',').split(\"' '\")\n",
    "            additional_states = [state.strip(\"'\") for state in additional_states]\n",
    "            states.extend(additional_states)\n",
    "\n",
    "    if current_char is not None:\n",
    "        statelabels[current_char] = states\n",
    "\n",
    "    return statelabels\n",
    "\n",
    "# Function to combine CHARLABELS and STATELABELS into a single dictionary\n",
    "def combine_labels_and_states(charlabels, statelabels):\n",
    "    character_info = {}\n",
    "    for char_index, description in charlabels.items():\n",
    "        states = statelabels.get(char_index, [])\n",
    "        state_dict = {str(i + 1): state for i, state in enumerate(states)}\n",
    "        character_info[str(char_index)] = {\n",
    "            \"description\": description,\n",
    "            \"states\": state_dict\n",
    "        }\n",
    "    return character_info\n",
    "\n",
    "# Function to extract sections from the NEXUS file content\n",
    "def extract_nexus_sections(nexus_content):\n",
    "    charlabels_content = \"\"\n",
    "    statelabels_content = \"\"\n",
    "    lines = nexus_content.strip().split(\"\\n\")\n",
    "    in_charlabels = False\n",
    "    in_statelabels = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"CHARLABELS\" in line:\n",
    "            in_charlabels = True\n",
    "            continue\n",
    "        if \"STATELABELS\" in line:\n",
    "            in_statelabels = True\n",
    "            continue\n",
    "        if \";\" in line:\n",
    "            in_charlabels = False\n",
    "            in_statelabels = False\n",
    "\n",
    "        if in_charlabels:\n",
    "            charlabels_content += line + \"\\n\"\n",
    "        if in_statelabels:\n",
    "            statelabels_content += line + \"\\n\"\n",
    "\n",
    "    return charlabels_content, statelabels_content\n",
    "\n",
    "# Function to parse the NEXUS file and return character information\n",
    "def parse_nexus_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        nexus_content = file.read()\n",
    "\n",
    "    charlabels_content, statelabels_content = extract_nexus_sections(nexus_content)\n",
    "\n",
    "    # Parse CHARLABELS section\n",
    "    charlabels = parse_charlabels(charlabels_content)\n",
    "\n",
    "    # Parse STATELABELS section\n",
    "    statelabels = parse_statelabels(statelabels_content)\n",
    "\n",
    "    # Combine parsed results into a character_info dictionary\n",
    "    character_info = combine_labels_and_states(charlabels, statelabels)\n",
    "\n",
    "    return character_info\n",
    "\n",
    "def load_prompt_messages(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Example Usage\n",
    "nexus_file_path = \"<Full path to the input Nexus file>\"\n",
    "csv_output_path = \"<Full path to the CSV output file>\"\n",
    "json_output_path = \"<Full path to the JSON output file>\"\n",
    "prompt_file_path = \"<Full path to the input Prompt file>\"\n",
    "\n",
    "# Step 1: Convert NEXUS to CSV and build knowledge graph\n",
    "knowledge_graph = nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path)\n",
    "\n",
    "# Step 2: Parse Nexus file to get character info\n",
    "character_info = parse_nexus_file(nexus_file_path)\n",
    "\n",
    "# Step 3: upload the prompt information\n",
    "prompt_messages = load_prompt_messages(prompt_file_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"Knowledge Graph:\")\n",
    "print(json.dumps(knowledge_graph, indent=4, ensure_ascii=False))\n",
    "print(\"\\nCharacter Info:\")\n",
    "print(json.dumps(character_info, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdce30a9b155b3cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第二个步骤是选择初始的分类 character ，并对这个初始的character信息进行解码，存储以字典格式，这样做的目的是可以减少API对数据的处理量，同时为后续的递归循环提供便捷"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ccb80a46c06096f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Input the API key and morphological matrix\n",
    "\n",
    "# Initialize the OpenAI client with the API key from environment variables\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Set up the prompt for the API input using the client.chat.completions.create interface\n",
    "# to conduct multi-turn conversations. Assign different roles in the conversation (user, assistant, system)\n",
    "# to ensure all input information is fully conveyed to the API model.\n",
    "# Replace the placeholder in the content template with actual data\n",
    "content_with_data = prompt_messages[\"initial_character_messages\"][3][\"content_template\"].format(\n",
    "        knowledge_graph=json.dumps(knowledge_graph)\n",
    "    )\n",
    "\n",
    "# Create messages list\n",
    "messages_initial = [\n",
    "        prompt_messages[\"initial_character_messages\"][0],\n",
    "        prompt_messages[\"initial_character_messages\"][1],\n",
    "        prompt_messages[\"initial_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data},\n",
    "    ]\n",
    "\n",
    "# Set various parameters to control the API response.\n",
    "# Setting the temperature to 0 and limiting max_tokens to save costs and avoid long, redundant outputs.\n",
    "initial_character_info = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages_initial,\n",
    "    stop=None,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Store the API call response results as a file.\n",
    "# (For subsequent distributed API call loops, consider storing in environment variables for continuous calls and modifications).\n",
    "initial_response = initial_character_info.choices[0].message.content\n",
    "\n",
    "# If used as a whole pipeline to transfer the results, ignore this print.\n",
    "# However, for debugging, you can use this print statement to check the response.\n",
    "print(initial_response)\n",
    "\n",
    "\n",
    "# Function to parse the classification result text into a dictionary format\n",
    "def parse_classification_result(result_text):\n",
    "    classification = {\"Character\": None, \"States\": {}}\n",
    "    try:\n",
    "        # Attempt to match the Character from the result text\n",
    "        character_match = re.search(r'\"Character\": \"([^\"]+)\"', result_text)\n",
    "        if character_match:\n",
    "            classification[\"Character\"] = character_match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"Character not found in the result text.\")\n",
    "\n",
    "        # Attempt to match each State and the corresponding species\n",
    "        state_sections = re.findall(r'\"(\\d+|[^\"]+)\":\\s*\\[(.*?)\\]', result_text)\n",
    "        if not state_sections:\n",
    "            raise ValueError(\"No states found in the result text.\")\n",
    "\n",
    "        for state, species_block in state_sections:\n",
    "            species_list = re.findall(r'\"([^\"]+)\"', species_block)\n",
    "            if not species_list:\n",
    "                raise ValueError(f\"No species found for state {state}.\")\n",
    "            classification[\"States\"][state] = species_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing classification result: {e}\")\n",
    "        # Decide whether to return an empty classification or raise an exception when an error occurs\n",
    "        raise e  # Or return classification\n",
    "\n",
    "    return classification\n",
    "\n",
    "# Parse the initial classification response from the API\n",
    "parsed_initial_classification = parse_classification_result(initial_response)\n",
    "print(parsed_initial_classification)\n",
    "\n",
    "# Function to generate groups from the classification result\n",
    "def generate_groups_from_classification(classification_result):\n",
    "    \"\"\"\n",
    "    Generate groups from classification result.\n",
    "\n",
    "    :param classification_result: Dictionary containing the classification result\n",
    "    :return: List of tuples, where each tuple contains a state and a list of species\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for state, species_list in classification_result[\"States\"].items():\n",
    "        groups.append((state, species_list))\n",
    "    return groups\n",
    "\n",
    "# Generate groups from the parsed initial classification\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(groups)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcf88ccbf6ea082"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第三个步骤是关于为后续每个组的结果进行进一步的分类，构建一个用于次级分类的API function，在此处的function中，我对于每个组进行分类时连续调用了两个API，这是因为在之前的测试中，在API的prompt中规范格式，以及使用API相关的function{type：JSON}都无法规范每次输出的格式结果，因此需要使用通过在调用一次API，来利用prompt设计一个JSON_API来帮助每次响应都能够获得相同格式的化的结果。同时在这个设计中只调用需要进行分类的groups字典结构中对应的物种，这样可以节省API的分析，便于产生更加高质量的结果"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c2b79bf38cb27d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 4\n",
    "\n",
    "# API call function for continued grouping for each subgroup\n",
    "def classify_group(group_species):\n",
    "    # Create a sub-matrix for the group of species\n",
    "    group_matrix = {species: knowledge_graph[species] for species in group_species}\n",
    "    group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"secondary_character_messages\"][3][\"content_template\"].format(\n",
    "        group_matrix_str=group_matrix_str\n",
    "    )\n",
    "\n",
    "    # Create messages list\n",
    "    messages_secondary = [\n",
    "        prompt_messages[\"secondary_character_messages\"][0],\n",
    "        prompt_messages[\"secondary_character_messages\"][1],\n",
    "        prompt_messages[\"secondary_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to classify the group\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_secondary,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        n=1\n",
    "    )\n",
    "    result_secondary = response.choices[0].message.content\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "        result_secondary=result_secondary\n",
    "    )\n",
    "\n",
    "    # Create messages_JSON list\n",
    "    messages_JSON1 = [\n",
    "        prompt_messages[\"JSON_format_messages\"][0],\n",
    "        prompt_messages[\"JSON_format_messages\"][1],\n",
    "        prompt_messages[\"JSON_format_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to format the response as JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_JSON1,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        n=1\n",
    "    )\n",
    "    json_result = response.choices[0].message.content\n",
    "    print(json_result)\n",
    "    return json_result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9809d1598bc014c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 这个部分是通过构建一个递归循环，来分别对于不同groups字典中存储的结果来进行API的调用，来获得对每个组别最终的分类情况，然后通过设置一个空列表再将这些产生的结果进行一个汇总，从而产生最终的对数据集中所有物种的基于形态学特征状态获得的分类结果（粗糙的分类检索表结果）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4824a5f4cf3334"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# part 5\n",
    "\n",
    "# Function to clean and extract JSON string\n",
    "def extract_json_string(json_string):\n",
    "    # Find the positions of the start and end of the JSON object\n",
    "    start = json_string.find('{')\n",
    "    end = json_string.rfind('}') + 1\n",
    "\n",
    "    # If both start and end positions are valid, extract and return the JSON string\n",
    "    if start != -1 and end != -1:\n",
    "        cleaned_string = json_string[start:end]\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    # If positions are not valid, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=10):\n",
    "    \"\"\"\n",
    "    Recursive classification function to process groups and store results.\n",
    "    :param groups: Groups to be processed\n",
    "    :param final_classification: Final classification result\n",
    "    :param classification_results: Classification results\n",
    "    :param depth: Current recursion depth\n",
    "    :param max_depth: Maximum recursion depth\n",
    "    :return: Final classification result\n",
    "    \"\"\"\n",
    "    # Continue looping while the groups list is not empty\n",
    "    # Initialize state and current_group for error handling\n",
    "    state, current_group = None, []\n",
    "    while groups:\n",
    "        try:\n",
    "            # Pop the first group from the list, getting the state and current group of species\n",
    "            state, current_group = groups.pop(0)\n",
    "            print(f\"Processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "\n",
    "            # If the current group has only one species, add it to the final classification\n",
    "            if len(current_group) == 1:\n",
    "                final_classification[current_group[0]] = current_group\n",
    "            # If the current recursion depth has reached the maximum depth, stop further classification\n",
    "            elif depth >= max_depth:\n",
    "                print(f\"Reached max depth {max_depth}. Stopping further classification for group: {current_group}\")\n",
    "                final_classification[state] = current_group\n",
    "            else:\n",
    "                # Call the classify_group function to classify the current group\n",
    "                classification_result = classify_group(current_group)\n",
    "                # Clean the API classification result to extract the JSON string\n",
    "                cleaned_classification_result = extract_json_string(classification_result)\n",
    "                # Store the classification result in classification_results\n",
    "                classification_results[state] = cleaned_classification_result\n",
    "\n",
    "                # Parse the classification result, create new subgroups, and add them to groups for further classification\n",
    "                parsed_result = parse_classification_result(classification_result)\n",
    "                new_groups = generate_groups_from_classification(parsed_result)\n",
    "\n",
    "                # Recursively call itself to process new subgroups, increasing the recursion depth\n",
    "                recursive_classification(new_groups, final_classification, classification_results, depth + 1, max_depth)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch exceptions and print error messages\n",
    "            print(f\"Error processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise e\n",
    "\n",
    "    return final_classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1866324dbc5e8c92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 在此处设计了主函数，用于直接调用之前所定义的这些function，然后通过最终的一个function（recursive_classification()）来产生最终的分类结果\n",
    "# **注意在此处需要确保之前存储initial character 分类结果的变量groups字典中存在实际的信息，这是因为在每次重新完成这整个过程时，这个groups的内容会被重新覆盖一遍，因此在整体的运行完成一遍所有的API之后，如果需要再次调用，需要考虑重新运行对初始API结果解析的代码，以确保groups中存在实际信息"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143aac6b3908d87d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 6\n",
    "\n",
    "# According print the 'groups' dictionary to make sure contain all initial character selection information\n",
    "print(groups)\n",
    "# If don't have any information, please using this function to reload the initial response information\n",
    "# groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "# If also none, please check whether exist the correct initial response information\n",
    "\n",
    "# Assume the variables have been initialized\n",
    "max_depth = 5  # Can be adjusted based on the hierarchical structure of input data and application requirements\n",
    "\n",
    "# Dictionary to store the final classification where each species is classified individually\n",
    "final_classification = {}\n",
    "\n",
    "# Dictionary to store the API classification results for each state\n",
    "classification_results = {}\n",
    "\n",
    "# Print the initial state of groups and dictionaries for debugging purposes\n",
    "print(\"Initial groups:\", groups)\n",
    "print(\"Initial final_classification:\", final_classification)\n",
    "print(\"Initial classification_results:\", classification_results)\n",
    "\n",
    "# Call the recursive_classification function to process the groups and store the results\n",
    "final_classification = recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=max_depth)\n",
    "\n",
    "# Print the final classification results\n",
    "print(\"Final Classification:\")\n",
    "print(json.dumps(final_classification, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the classification results from the API calls\n",
    "print(\"\\nClassification Results:\")\n",
    "print(classification_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f7d855b1816c5e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 在获得了最终的分类结果后，此处存在一个检查的程序，来帮助消除人工智能幻觉，避免出现额外的错误的环节。由于在后续的检查步骤中需要使用到groups中的信息，而在上面的分类结果中已经被调用groups中的信息已经在调用API的过程中被覆盖了，因此我需要通过使用之前解析groups的函数，再解析一次initial response，来保证groups中存在初始character的分类结果，且为正确的字典格式"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3cfccc8dab5231"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reload the groups information to help with further processing\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(classification_results)\n",
    "print(type(classification_results))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53902333a3f376fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 此处进入检查部分\n",
    "可以在这个地方展示检查纠正部分的流程图\n",
    "# 此处是通过解析之前所产生的最终的分类结果，将其为每个species选择的character和state分别存储为一个JSON格式的列表，方便与之前knowledge graph（JSON格式）的原始数据进行对比，来检查是否存在不一样的地方"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198c492013e90731"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 7\n",
    "\n",
    "# Function to extract paths from the classification tree\n",
    "def extract_paths(node, path=None):\n",
    "    if path is None:\n",
    "        path = {}\n",
    "\n",
    "    if 'Character' in node and 'States' in node:\n",
    "        current_character = node['Character'].replace(\" \", \"\").strip()\n",
    "        for state, value in node['States'].items():\n",
    "            new_path = path.copy()\n",
    "            new_path[current_character] = state\n",
    "            if isinstance(value, dict):\n",
    "                yield from extract_paths(value, new_path)\n",
    "            else:\n",
    "                for species in value:\n",
    "                    yield species, new_path\n",
    "\n",
    "# Process each classification result and extract paths\n",
    "final_results = {}\n",
    "\n",
    "for key, json_str in classification_results.items():\n",
    "    classification_data = json.loads(json_str)\n",
    "    species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "    formatted_results = {}\n",
    "    for species, path in species_paths:\n",
    "        formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "    final_results[key] = formatted_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8907377452a741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 在这个部分是通过将之前的不一样的结果记录下来，如果在和原始的character & state信息存在不同的地方，表明API产生的分类结果存在错误，准确的记录下来这些错误分别出自于groups结果中的那个键中，来以便于后续再调用纠错API时能够只对这个键的结果进行分析，节省调用API的使用成本和使用次数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b478fb4d4becf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 8\n",
    "\n",
    "# Function to check if the state matches the correct state\n",
    "def check_state_match(state, correct_state):\n",
    "    if correct_state is None:\n",
    "        return False\n",
    "    if \" and \" in correct_state:\n",
    "        correct_states = correct_state.split(\" and \")\n",
    "        return all(sub_state in correct_states for sub_state in state.split(\" and \"))\n",
    "    return state == correct_state\n",
    "\n",
    "# Validate classification results and log errors\n",
    "def validate_results(final_results, knowledge_graph):\n",
    "    errors = []\n",
    "    for key, results in final_results.items():\n",
    "        for species, data in results.items():\n",
    "            if species in knowledge_graph:\n",
    "                mismatch = False\n",
    "                incorrect_character_states = {}\n",
    "                for character, state in data[\"Characteristics\"].items():\n",
    "                    character = character.replace(\" \", \"\").strip()\n",
    "                    correct_state = knowledge_graph[species][\"Characteristics\"].get(character)\n",
    "                    if correct_state is None or not check_state_match(state, correct_state):\n",
    "                        mismatch = True\n",
    "                        incorrect_character_states[character] = {\"error_state\": state, \"correct_state\": correct_state}\n",
    "                if mismatch:\n",
    "                    errors.append({\n",
    "                        \"species\": species,\n",
    "                        \"key\": key,\n",
    "                        \"error\": \"Mismatch\",\n",
    "                        \"error_result\": incorrect_character_states,\n",
    "                        \"correct_result\": {character: knowledge_graph[species][\"Characteristics\"].get(character) for character in incorrect_character_states}\n",
    "                    })\n",
    "            else:\n",
    "                errors.append({\n",
    "                    \"species\": species,\n",
    "                    \"key\": key,\n",
    "                    \"error\": \"Species not found in knowledge graph\",\n",
    "                    \"error_result\": data[\"Characteristics\"]\n",
    "                })\n",
    "    return errors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b4f5c241b6f9bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 此处是创建一个纠错API，通过将之前记录下来的错误信息保留下来，确定这个错误结果是groups中的哪个键，将这个错误以及纠正的信息传递回API，然后在只输入这个键相关的形态学矩阵信息来进行纠错API（但是即使这样这个API也可能存在一个问题会调用多次结果，因为LLM很固执，他不一定能基于你的错误信息就越一定能获得正确的反馈）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449f9ad5e50987c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 9\n",
    "\n",
    "# Function to get the species list for a specific state from the groups\n",
    "def get_species_list_for_state(groups, key):\n",
    "    species_list = []\n",
    "    for state, species in groups:\n",
    "        if state == key:\n",
    "            species_list = species\n",
    "            break\n",
    "    if not species_list:\n",
    "        print(f\"Key {key} not found in groups\")\n",
    "    else:\n",
    "        print(f\"Processing species list for state '{key}': {species_list}\")\n",
    "    return species_list\n",
    "\n",
    "\n",
    "# Part 10\n",
    "\n",
    "# Function to correct classification errors using the API\n",
    "def correct_classification(errors, classification_results, knowledge_graph):\n",
    "    for error in errors:\n",
    "        key = error['key']\n",
    "\n",
    "        # Get the species list for the erroneous state\n",
    "        species_list = get_species_list_for_state(groups, key)\n",
    "        if not species_list:\n",
    "            continue\n",
    "\n",
    "        # Create a sub-matrix for the group of species\n",
    "        group_matrix = {s: knowledge_graph[s] for s in species_list}\n",
    "        group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "        # Replace the placeholders in the content templates with actual data\n",
    "        content_error = prompt_messages[\"correct_messages\"][2][\"content_template\"].format(error=error)\n",
    "        content_group_matrix = prompt_messages[\"correct_messages\"][4][\"content_template\"].format(group_matrix_str=group_matrix_str)\n",
    "\n",
    "        # Create messages list\n",
    "        messages_correct = [\n",
    "            prompt_messages[\"correct_messages\"][0],\n",
    "            prompt_messages[\"correct_messages\"][1],\n",
    "            {\"role\": \"user\", \"content\": content_error},\n",
    "            prompt_messages[\"correct_messages\"][3],\n",
    "            {\"role\": \"user\", \"content\": content_group_matrix}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to correct the classification\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_correct,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "            n=1\n",
    "        )\n",
    "        corrected_result = response.choices[0].message.content\n",
    "\n",
    "        # Replace the placeholder in the content template with actual data\n",
    "        content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "            result_secondary=corrected_result\n",
    "        )\n",
    "\n",
    "        # Create messages_JSON list\n",
    "        messages_JSON2 = [\n",
    "            prompt_messages[\"JSON_format_messages\"][0],\n",
    "            prompt_messages[\"JSON_format_messages\"][1],\n",
    "            prompt_messages[\"JSON_format_messages\"][2],\n",
    "            {\"role\": \"user\", \"content\": content_with_data}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to format the response as JSON\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_JSON2,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1500,\n",
    "            n=1\n",
    "        )\n",
    "        json_result = response.choices[0].message.content\n",
    "        json_cleaned_result = extract_json_string(json_result)\n",
    "        print(json_cleaned_result)\n",
    "        classification_results[key] = json_cleaned_result\n",
    "        return classification_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8587edde23dcdb2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 这个部分是设置了一个关于纠错部分的while循环，因为涉及到API关于LLM的人工智能幻觉，因此纠错API即使存在错误的API结果的情况下也不一定能一次就准确的获得正确的答案，因此需要一个不断纠错改正的过程，然后再每次对纠错的结果在调用之前的函数步骤进行检查，来检查使用的species的character以及state是否存在和原始数据集不一样的地方，如果一样继续记录这个错误，然后传递调用这个纠错API，，直至最终产生的结果与原始的character和state信息没有差别，就停止这些步骤，生成最终完整正确的结果"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "111c0434c4cb0f84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 11\n",
    "\n",
    "# Validate the initial classification results and log any errors\n",
    "errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Purpose: Enter a loop until all errors have been fixed.\n",
    "# Function: Executes the code inside the loop when the errors list is not empty.\n",
    "while errors:\n",
    "    # Fix current categorization errors using the API\n",
    "    classification_results = correct_classification(errors, classification_results, knowledge_graph)\n",
    "\n",
    "    # Reset the final_results dictionary to store the corrected categorization results\n",
    "    final_results = {}\n",
    "\n",
    "    # Iterate over the corrected classification results and extract species classification paths\n",
    "    for key, json_str in classification_results.items():\n",
    "        classification_data = json.loads(json_str)\n",
    "        species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "        # Format the extracted classification paths and store them in the formatted_results dictionary\n",
    "        formatted_results = {}\n",
    "        for species, path in species_paths:\n",
    "            formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "        # Add the formatted classification results to final_results\n",
    "        final_results[key] = formatted_results\n",
    "\n",
    "    # Re-validate the corrected classification results and log any remaining errors\n",
    "    errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Save the final classification results to a JSON file\n",
    "with open('final_classification.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(\"Final classification results have been saved to 'final_classification.json'.\")\n",
    "print(json.dumps(final_results, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "895880990113e7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 由于分类检索表的格式实际上是存在对应的character和state的真实信息的，即对应character和state的描述，在最开始的过程中，我们解析了Nexus file中的character label和 state label，获得了每个character和state的描述信息，因此需要进行character mapping将对应的描述信息传递上去"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3315aded8b2352c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 12\n",
    "\n",
    "# Convert classification_results JSON strings to dictionaries\n",
    "classification_result = {key: json.loads(value) for key, value in classification_results.items()}\n",
    "\n",
    "\n",
    "# Recursive function to convert structure to the desired format\n",
    "def convert_structure(node):\n",
    "    if \"Character\" in node and \"States\" in node:\n",
    "        character = node[\"Character\"]\n",
    "        states = node[\"States\"]\n",
    "        converted = {f\"Character {character.replace('Character', '')}\": {}}\n",
    "        for state, sub_node in states.items():\n",
    "            state_key = f\"State {state}\"\n",
    "            if isinstance(sub_node, list):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = sub_node[0] if len(sub_node) == 1 else sub_node\n",
    "            elif isinstance(sub_node, dict):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = convert_structure(sub_node)\n",
    "        return converted\n",
    "    return node\n",
    "\n",
    "\n",
    "# Process classification results to the desired format\n",
    "converted_result = {}\n",
    "for key, value in classification_result.items():\n",
    "    converted_result[f\"Character {key}\"] = convert_structure(value)\n",
    "\n",
    "\n",
    "# Combine initial classification with other results\n",
    "def combine_results(initial, secondary, state_key):\n",
    "    if not secondary:\n",
    "        return\n",
    "\n",
    "    initial_states = initial[\"States\"].get(state_key)\n",
    "    if initial_states is None:\n",
    "        initial[\"States\"][state_key] = secondary\n",
    "        return\n",
    "\n",
    "    if isinstance(initial_states, list):\n",
    "        if isinstance(secondary, list):\n",
    "            initial[\"States\"][state_key] = list(set(initial_states + secondary))  # Merge two lists and remove duplicates\n",
    "        else:\n",
    "            initial[\"States\"][state_key] = secondary\n",
    "    elif isinstance(initial_states, dict):\n",
    "        if isinstance(secondary, dict):\n",
    "            for key, value in secondary[\"States\"].items():\n",
    "                if key not in initial_states:\n",
    "                    initial_states[key] = value\n",
    "                else:\n",
    "                    combine_results(initial_states, value, key)\n",
    "        else:\n",
    "            raise ValueError(f\"Conflicting types for key {state_key}: {type(initial_states)} vs {type(secondary)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for initial states: {type(initial_states)}\")\n",
    "\n",
    "\n",
    "# Dynamically combine all secondary classification results\n",
    "for state_key, secondary in classification_result.items():\n",
    "    combine_results(parsed_initial_classification, secondary, state_key)\n",
    "\n",
    "# Convert the merged results to the desired format\n",
    "converted_initial_classification = convert_structure(parsed_initial_classification)\n",
    "\n",
    "# Recursive function to replace indices with descriptions in the classification key\n",
    "def replace_indices_with_descriptions_in_key(key, character_info, parent_char_index=None):\n",
    "    updated_key = {}\n",
    "    for char_state, subtree in key.items():\n",
    "        if char_state.startswith(\"Character\"):\n",
    "            parts = char_state.split()\n",
    "            if len(parts) > 1:\n",
    "                char_index = parts[1]\n",
    "                if char_index in character_info:\n",
    "                    char_description = f\"Character {char_index}: {character_info[char_index]['description']}\"\n",
    "                    if isinstance(subtree, dict):\n",
    "                        updated_subtree = replace_indices_with_descriptions_in_key(subtree, character_info, char_index)\n",
    "                        updated_key[char_description] = updated_subtree\n",
    "                    else:\n",
    "                        updated_key[char_description] = subtree\n",
    "                else:\n",
    "                    updated_key[char_state] = subtree\n",
    "            else:\n",
    "                updated_key[char_state] = subtree\n",
    "        elif char_state.startswith(\"State\") and parent_char_index:\n",
    "            states = char_state.split()[1:]\n",
    "            state_descriptions = []\n",
    "            for state in states:\n",
    "                individual_states = state.split(\"and\")\n",
    "                descriptions = [character_info[parent_char_index][\"states\"].get(s.strip(), \"\") for s in individual_states]\n",
    "                state_descriptions.append(\" and \".join(filter(None, descriptions)))\n",
    "            state_key = f\"State {' '.join(states)}: {' / '.join(state_descriptions)}\"\n",
    "            if isinstance(subtree, dict):\n",
    "                updated_key[state_key] = replace_indices_with_descriptions_in_key(subtree, character_info, parent_char_index)\n",
    "            else:\n",
    "                updated_key[state_key] = subtree\n",
    "        else:\n",
    "            updated_key[char_state] = subtree\n",
    "    return updated_key\n",
    "\n",
    "\n",
    "# Replace feature and state descriptions\n",
    "updated_classification_key = replace_indices_with_descriptions_in_key(converted_initial_classification, character_info)\n",
    "\n",
    "# Print the updated classification key\n",
    "print(\"Updated Classification Key:\")\n",
    "print(json.dumps(updated_classification_key, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d65fd4d4cf7fda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 同时由于输出的最终格式均为JSON格式，很好的使用了嵌套关系来展示最终生成的分类检索表，但是这个分类检索表的结果并不直观，因此为了获得一个更好看，更简洁的分类检索表，最终可以使用这个部分来优化最终结果的格式"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3433a36d44353582"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 13\n",
    "\n",
    "# Example initial result\n",
    "initial_result = parsed_initial_classification\n",
    "\n",
    "# Parse the API response JSON strings into dictionaries\n",
    "parsed_classification_results = {key: json.loads(value) for key, value in classification_results.items()}\n",
    "\n",
    "# Function to combine the initial and secondary classification results\n",
    "def combine_results(initial, secondary, state_key):\n",
    "    if not secondary:\n",
    "        return\n",
    "\n",
    "    initial_states = initial[\"States\"].get(state_key)\n",
    "    if initial_states is None:\n",
    "        initial[\"States\"][state_key] = secondary\n",
    "        return\n",
    "\n",
    "    if isinstance(initial_states, list):\n",
    "        if isinstance(secondary, list):\n",
    "            initial[\"States\"][state_key] = list(set(initial_states + secondary))  # Merge two lists and remove duplicates\n",
    "        else:\n",
    "            initial[\"States\"][state_key] = secondary\n",
    "    elif isinstance(initial_states, dict):\n",
    "        if isinstance(secondary, dict):\n",
    "            for key, value in secondary[\"States\"].items():\n",
    "                if key not in initial_states:\n",
    "                    initial_states[key] = value\n",
    "                else:\n",
    "                    combine_results(initial_states, value, key)\n",
    "        else:\n",
    "            raise ValueError(f\"Conflicting types for key {state_key}: {type(initial_states)} vs {type(secondary)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for initial states: {type(initial_states)}\")\n",
    "\n",
    "\n",
    "# Dynamically combine all secondary classification results into the initial result\n",
    "for state_key, secondary in parsed_classification_results.items():\n",
    "    combine_results(initial_result, secondary, state_key)\n",
    "\n",
    "\n",
    "# Function to display the final classification result in a readable format\n",
    "def display_classification(result, indent=0):\n",
    "    indent_space = \" \" * indent\n",
    "    character = result.get(\"Character\")\n",
    "    states = result.get(\"States\")\n",
    "\n",
    "    classification = {}\n",
    "    if character and states:\n",
    "        classification[\"Character\"] = character\n",
    "        classification[\"States\"] = {}\n",
    "        print(f\"{indent_space}1. **{character}:**\")\n",
    "        for state, species in states.items():\n",
    "            if isinstance(species, list):\n",
    "                print(f\"{indent_space}   - State \\\"{state}\\\": {', '.join(species)}\")\n",
    "                classification[\"States\"][state] = species\n",
    "            elif isinstance(species, dict):\n",
    "                print(f\"{indent_space}   - State \\\"{state}\\\":\")\n",
    "                classification[\"States\"][state] = display_classification(species, indent + 4)\n",
    "    return classification\n",
    "\n",
    "# Display the final classification result\n",
    "final_result = display_classification(initial_result)\n",
    "# Uncomment the lines below to print the final result as JSON\n",
    "# print(\"\\nFinal Result JSON:\")\n",
    "# print(json.dumps(final_result, indent=2))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ce671d86564c87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
