{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Taxon_GPT_API**\n",
    "## Introduction\n",
    "Taxonomy, as a branch of systematics, holds critical importance across various biological disciplines. Accurate taxonomic information is essential for taxonomists to analyze evolutionary relationships between species, assess morphological characteristics, and name new species. However, this process heavily relies on natural language and involves extensive manual work to handle taxonomic data, consuming significant time and human resources. Large Language Models (LLMs) have demonstrated excellent performance in Natural Language Processing (NLP). In this manuscript, we demonstrated the GPT-4o model, a efficient LLM, can effectively handling natural language in taxonomic research, using relevant data to generate taxonomically meaningful results. We developed the **Taxon-GPT-API** function, which utilizes the GPT-4o model to process Nexus matrix data, converting it into taxonomic keys and taxonomic descriptions, providing an innovative automated approach to taxonomic data processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90219027de64232"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Need a section on how to properly obtain an OpenAi API_key for subsequent API calls."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad2c4d9349035822"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# How to call this key in the Python environment, using this API as an environment variable in the terminal or powershell"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "159ef41b927eeb9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part to start with is about using loads related to installing packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6598279122552a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "from openai import OpenAI  # For interacting with OpenAI API\n",
    "import os  # For interacting with the operating system, such as file paths\n",
    "import re  # For regular expressions, useful for pattern matching in strings\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is importing the files that will be used, parsing the Nexus files and converting them to various file types.\n",
    "(I can insert some of the flowcharts I made earlier in this section to help with understanding)\n",
    "1. Import files: Nexus files, Prompt files.\n",
    "2. Output files: csv file, knowledge_graph JSON file, character_info file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6e1a53b835d9d6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 1\n",
    "\n",
    "# Function to convert a letter to a number based on its position in the alphabet\n",
    "def letter_to_number(letter):\n",
    "    return str(ord(letter) - ord('A') + 10)\n",
    "\n",
    "\n",
    "# Function to parse the matrix content from NEXUS format\n",
    "def parse_matrix(matrix_content):\n",
    "    data = []\n",
    "    headers = []\n",
    "    lines = matrix_content.strip().split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        taxa = lines[i].strip().strip(\"'\")  # Extract taxa name\n",
    "        traits = lines[i + 1].strip()  # Extract traits for the taxa\n",
    "        species_traits = []\n",
    "        j = 0\n",
    "        while j < len(traits):\n",
    "            if traits[j] == '(':  # Handle compound states\n",
    "                j += 1\n",
    "                states = ''\n",
    "                while traits[j] != ')':\n",
    "                    if traits[j].isalpha():\n",
    "                        states += letter_to_number(traits[j])\n",
    "                    else:\n",
    "                        states += traits[j]\n",
    "                    j += 1\n",
    "                species_traits.append(','.join(states))\n",
    "            elif traits[j] == '?':\n",
    "                species_traits.append('Missing')  # Missing data\n",
    "            elif traits[j] == '-':\n",
    "                species_traits.append('Not Applicable')  # Not applicable data\n",
    "            elif traits[j].isalpha():\n",
    "                species_traits.append(letter_to_number(traits[j]))  # Convert letter to number\n",
    "            else:\n",
    "                species_traits.append(traits[j])  # Directly append the trait\n",
    "            j += 1\n",
    "        data.append([taxa] + species_traits)  # Append the parsed traits\n",
    "    max_traits = max(len(row) - 1 for row in data)\n",
    "    headers = ['taxa'] + [f'Character{i + 1}' for i in range(max_traits)]  # Create headers for DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=headers)  # Create DataFrame\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to convert NEXUS file to CSV format\n",
    "def convert_nexus_to_csv(file_path, output_path):\n",
    "    try:\n",
    "        encodings = ['utf-8', 'gbk', 'latin1']  # List of encodings to try\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file with encoding: {encoding}\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to read file with encoding: {encoding}\")\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"Failed to read file with all attempted encodings.\")\n",
    "\n",
    "        # Extract the MATRIX section from the NEXUS file content\n",
    "        matrix_content = re.search(r'MATRIX\\s*(.*?)\\s*;', content, re.DOTALL).group(1).strip()\n",
    "        df = parse_matrix(matrix_content)  # Parse the matrix content into a DataFrame\n",
    "        df.to_csv(output_path, index=False)  # Save DataFrame as CSV\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Function to build a knowledge graph from the parsed matrix\n",
    "def build_knowledge_graph(matrix):\n",
    "    knowledge_graph = {}\n",
    "    for _, row in matrix.iterrows():\n",
    "        taxa = row.iloc[0]\n",
    "        characteristics = {}\n",
    "        for col in matrix.columns[1:]:\n",
    "            state = row[col]\n",
    "            if isinstance(state, str) and ',' in state:\n",
    "                state = state.replace(',', ' and ')\n",
    "            characteristics[col] = str(state)\n",
    "        knowledge_graph[taxa] = {'Characteristics': characteristics}\n",
    "    return knowledge_graph\n",
    "\n",
    "\n",
    "# Function to save the knowledge graph as a JSON file\n",
    "def save_knowledge_graph_as_json(knowledge_graph, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(knowledge_graph, f, indent=4)\n",
    "\n",
    "# Function to convert NEXUS to knowledge graph and save as CSV and JSON\n",
    "def nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path):\n",
    "    # Step 1: Convert NEXUS to CSV\n",
    "    df = convert_nexus_to_csv(nexus_file_path, csv_output_path)\n",
    "\n",
    "    if df is not None:\n",
    "        # Step 2: Build the knowledge graph from the DataFrame\n",
    "        knowledge_graph = build_knowledge_graph(df)\n",
    "\n",
    "        # Step 3: Save the knowledge graph as a JSON file\n",
    "        save_knowledge_graph_as_json(knowledge_graph, json_output_path)\n",
    "\n",
    "        return knowledge_graph\n",
    "    else:\n",
    "        print(\"Failed to create the DataFrame from NEXUS file.\")\n",
    "        return None\n",
    "\n",
    "# Function to parse CHARLABELS section of the NEXUS file\n",
    "def parse_charlabels(charlabels_content):\n",
    "    charlabels = {}\n",
    "    lines = charlabels_content.strip().split(\"\\n\")\n",
    "    char_pattern = re.compile(r\"\\[(\\d+)\\(\\d+\\)\\]\\s+'(.+?)'\")\n",
    "    for line in lines:\n",
    "        match = char_pattern.match(line.strip().rstrip(','))\n",
    "        if match:\n",
    "            char_index = int(match.group(1))\n",
    "            description = match.group(2)\n",
    "            charlabels[char_index] = description\n",
    "    return charlabels\n",
    "\n",
    "# Function to parse STATELABELS section of the NEXUS file\n",
    "def parse_statelabels(statelabels_content):\n",
    "    statelabels = {}\n",
    "    lines = statelabels_content.strip().split(\"\\n\")\n",
    "    current_char = None\n",
    "    states = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r'^\\d+', line):\n",
    "            if current_char is not None:\n",
    "                statelabels[current_char] = states\n",
    "            parts = line.split(' ', 1)\n",
    "            current_char = int(parts[0])\n",
    "            states = parts[1].strip().strip(',').split(\"' '\")\n",
    "            states = [state.strip(\"'\") for state in states]\n",
    "        else:\n",
    "            additional_states = line.strip().strip(',').split(\"' '\")\n",
    "            additional_states = [state.strip(\"'\") for state in additional_states]\n",
    "            states.extend(additional_states)\n",
    "\n",
    "    if current_char is not None:\n",
    "        statelabels[current_char] = states\n",
    "\n",
    "    return statelabels\n",
    "\n",
    "# Function to combine CHARLABELS and STATELABELS into a single dictionary\n",
    "def combine_labels_and_states(charlabels, statelabels):\n",
    "    character_info = {}\n",
    "    for char_index, description in charlabels.items():\n",
    "        states = statelabels.get(char_index, [])\n",
    "        state_dict = {str(i + 1): state for i, state in enumerate(states)}\n",
    "        character_info[str(char_index)] = {\n",
    "            \"description\": description,\n",
    "            \"states\": state_dict\n",
    "        }\n",
    "    return character_info\n",
    "\n",
    "# Function to extract sections from the NEXUS file content\n",
    "def extract_nexus_sections(nexus_content):\n",
    "    charlabels_content = \"\"\n",
    "    statelabels_content = \"\"\n",
    "    lines = nexus_content.strip().split(\"\\n\")\n",
    "    in_charlabels = False\n",
    "    in_statelabels = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"CHARLABELS\" in line:\n",
    "            in_charlabels = True\n",
    "            continue\n",
    "        if \"STATELABELS\" in line:\n",
    "            in_statelabels = True\n",
    "            continue\n",
    "        if \";\" in line:\n",
    "            in_charlabels = False\n",
    "            in_statelabels = False\n",
    "\n",
    "        if in_charlabels:\n",
    "            charlabels_content += line + \"\\n\"\n",
    "        if in_statelabels:\n",
    "            statelabels_content += line + \"\\n\"\n",
    "\n",
    "    return charlabels_content, statelabels_content\n",
    "\n",
    "# Function to parse the NEXUS file and return character information\n",
    "def parse_nexus_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        nexus_content = file.read()\n",
    "\n",
    "    charlabels_content, statelabels_content = extract_nexus_sections(nexus_content)\n",
    "\n",
    "    # Parse CHARLABELS section\n",
    "    charlabels = parse_charlabels(charlabels_content)\n",
    "\n",
    "    # Parse STATELABELS section\n",
    "    statelabels = parse_statelabels(statelabels_content)\n",
    "\n",
    "    # Combine parsed results into a character_info dictionary\n",
    "    character_info = combine_labels_and_states(charlabels, statelabels)\n",
    "\n",
    "    return character_info\n",
    "\n",
    "def load_prompt_messages(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Example Usage\n",
    "nexus_file_path = \"<Full path to the input Nexus file>\"\n",
    "csv_output_path = \"<Full path to the CSV output file>\"\n",
    "json_output_path = \"<Full path to the JSON output file>\"\n",
    "prompt_file_path = \"<Full path to the input Prompt file>\"\n",
    "\n",
    "# Step 1: Convert NEXUS to CSV and build knowledge graph\n",
    "knowledge_graph = nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path)\n",
    "\n",
    "# Step 2: Parse Nexus file to get character info\n",
    "character_info = parse_nexus_file(nexus_file_path)\n",
    "\n",
    "# Step 3: upload the prompt information\n",
    "prompt_messages = load_prompt_messages(prompt_file_path)\n",
    "\n",
    "# Display the results\n",
    "print(\"Knowledge Graph:\")\n",
    "print(json.dumps(knowledge_graph, indent=4, ensure_ascii=False))\n",
    "print(\"\\nCharacter Info:\")\n",
    "print(json.dumps(character_info, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2b89ed3d6fdbe5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second step is to select an initial categorized character and decode this initial character information and store it in a dictionary format. The purpose of doing this is to reduce the amount of data that the API has to process, and at the same time provide a convenient way for subsequent recursive loops to occur."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ccb80a46c06096f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Input the API key and morphological matrix\n",
    "\n",
    "# Initialize the OpenAI client with the API key from environment variables\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Set up the prompt for the API input using the client.chat.completions.create interface\n",
    "# to conduct multi-turn conversations. Assign different roles in the conversation (user, assistant, system)\n",
    "# to ensure all input information is fully conveyed to the API model.\n",
    "# Replace the placeholder in the content template with actual data\n",
    "content_with_data = prompt_messages[\"initial_character_messages\"][3][\"content_template\"].format(\n",
    "        knowledge_graph=json.dumps(knowledge_graph)\n",
    "    )\n",
    "\n",
    "# Create messages list\n",
    "messages_initial = [\n",
    "        prompt_messages[\"initial_character_messages\"][0],\n",
    "        prompt_messages[\"initial_character_messages\"][1],\n",
    "        prompt_messages[\"initial_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data},\n",
    "    ]\n",
    "\n",
    "# Set various parameters to control the API response.\n",
    "# Setting the temperature to 0 and limiting max_tokens to save costs and avoid long, redundant outputs.\n",
    "initial_character_info = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages_initial,\n",
    "    stop=None,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Store the API call response results as a file.\n",
    "# (For subsequent distributed API call loops, consider storing in environment variables for continuous calls and modifications).\n",
    "initial_response = initial_character_info.choices[0].message.content\n",
    "\n",
    "# If used as a whole pipeline to transfer the results, ignore this print.\n",
    "# However, for debugging, you can use this print statement to check the response.\n",
    "print(initial_response)\n",
    "\n",
    "\n",
    "# Function to parse the classification result text into a dictionary format\n",
    "def parse_classification_result(result_text):\n",
    "    classification = {\"Character\": None, \"States\": {}}\n",
    "    try:\n",
    "        # Attempt to match the Character from the result text\n",
    "        character_match = re.search(r'\"Character\": \"([^\"]+)\"', result_text)\n",
    "        if character_match:\n",
    "            classification[\"Character\"] = character_match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"Character not found in the result text.\")\n",
    "\n",
    "        # Attempt to match each State and the corresponding species\n",
    "        state_sections = re.findall(r'\"(\\d+|[^\"]+)\":\\s*\\[(.*?)\\]', result_text)\n",
    "        if not state_sections:\n",
    "            raise ValueError(\"No states found in the result text.\")\n",
    "\n",
    "        for state, species_block in state_sections:\n",
    "            species_list = re.findall(r'\"([^\"]+)\"', species_block)\n",
    "            if not species_list:\n",
    "                raise ValueError(f\"No species found for state {state}.\")\n",
    "            classification[\"States\"][state] = species_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing classification result: {e}\")\n",
    "        # Decide whether to return an empty classification or raise an exception when an error occurs\n",
    "        raise e  # Or return classification\n",
    "\n",
    "    return classification\n",
    "\n",
    "# Parse the initial classification response from the API\n",
    "parsed_initial_classification = parse_classification_result(initial_response)\n",
    "print(parsed_initial_classification)\n",
    "\n",
    "# Function to generate groups from the classification result\n",
    "def generate_groups_from_classification(classification_result):\n",
    "    \"\"\"\n",
    "    Generate groups from classification result.\n",
    "\n",
    "    :param classification_result: Dictionary containing the classification result\n",
    "    :return: List of tuples, where each tuple contains a state and a list of species\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for state, species_list in classification_result[\"States\"].items():\n",
    "        groups.append((state, species_list))\n",
    "    return groups\n",
    "\n",
    "# Generate groups from the parsed initial classification\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(groups)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcf88ccbf6ea082"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The third step involves further classification of the results for each subsequent group. I developed an API function for this secondary classification, where I call two APIs consecutively. This approach is due to previous tests where ensuring consistent output format using the API's prompt and function{type: JSON} was unsuccessful. Therefore, I designed a JSON_API to help achieve a consistent format in each response by calling the API once. This design also ensures that only the species within the relevant groups dictionary structure are classified, saving API analysis time and facilitating higher quality results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c2b79bf38cb27d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 3\n",
    "\n",
    "# API call function for continued grouping for each subgroup\n",
    "def classify_group(group_species):\n",
    "    # Create a sub-matrix for the group of species\n",
    "    group_matrix = {species: knowledge_graph[species] for species in group_species}\n",
    "    group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"secondary_character_messages\"][3][\"content_template\"].format(\n",
    "        group_matrix_str=group_matrix_str\n",
    "    )\n",
    "\n",
    "    # Create messages list\n",
    "    messages_secondary = [\n",
    "        prompt_messages[\"secondary_character_messages\"][0],\n",
    "        prompt_messages[\"secondary_character_messages\"][1],\n",
    "        prompt_messages[\"secondary_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to classify the group\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_secondary,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        n=1\n",
    "    )\n",
    "    result_secondary = response.choices[0].message.content\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "        result_secondary=result_secondary\n",
    "    )\n",
    "\n",
    "    # Create messages_JSON list\n",
    "    messages_JSON1 = [\n",
    "        prompt_messages[\"JSON_format_messages\"][0],\n",
    "        prompt_messages[\"JSON_format_messages\"][1],\n",
    "        prompt_messages[\"JSON_format_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to format the response as JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_JSON1,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        n=1\n",
    "    )\n",
    "    json_result = response.choices[0].message.content\n",
    "    print(json_result)\n",
    "    return json_result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9809d1598bc014c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section involves constructing a recursive loop to call the API for the results stored in different groups within the dictionary, obtaining the final classification for each group. An empty list is set up to aggregate these results, producing the final classification outcome for all species in the dataset based on morphological character states (a rough classification key)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4824a5f4cf3334"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# part 4\n",
    "\n",
    "# Function to clean and extract JSON string\n",
    "def extract_json_string(json_string):\n",
    "    # Find the positions of the start and end of the JSON object\n",
    "    start = json_string.find('{')\n",
    "    end = json_string.rfind('}') + 1\n",
    "\n",
    "    # If both start and end positions are valid, extract and return the JSON string\n",
    "    if start != -1 and end != -1:\n",
    "        cleaned_string = json_string[start:end]\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    # If positions are not valid, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=10):\n",
    "    \"\"\"\n",
    "    Recursive classification function to process groups and store results.\n",
    "    :param groups: Groups to be processed\n",
    "    :param final_classification: Final classification result\n",
    "    :param classification_results: Classification results\n",
    "    :param depth: Current recursion depth\n",
    "    :param max_depth: Maximum recursion depth\n",
    "    :return: Final classification result\n",
    "    \"\"\"\n",
    "    # Continue looping while the groups list is not empty\n",
    "    # Initialize state and current_group for error handling\n",
    "    state, current_group = None, []\n",
    "    while groups:\n",
    "        try:\n",
    "            # Pop the first group from the list, getting the state and current group of species\n",
    "            state, current_group = groups.pop(0)\n",
    "            print(f\"Processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "\n",
    "            # If the current group has only one species, add it to the final classification\n",
    "            if len(current_group) == 1:\n",
    "                final_classification[current_group[0]] = current_group\n",
    "            # If the current recursion depth has reached the maximum depth, stop further classification\n",
    "            elif depth >= max_depth:\n",
    "                print(f\"Reached max depth {max_depth}. Stopping further classification for group: {current_group}\")\n",
    "                final_classification[state] = current_group\n",
    "            else:\n",
    "                # Call the classify_group function to classify the current group\n",
    "                classification_result = classify_group(current_group)\n",
    "                # Clean the API classification result to extract the JSON string\n",
    "                cleaned_classification_result = extract_json_string(classification_result)\n",
    "                # Store the classification result in classification_results\n",
    "                classification_results[state] = cleaned_classification_result\n",
    "\n",
    "                # Parse the classification result, create new subgroups, and add them to groups for further classification\n",
    "                parsed_result = parse_classification_result(classification_result)\n",
    "                new_groups = generate_groups_from_classification(parsed_result)\n",
    "\n",
    "                # Recursively call itself to process new subgroups, increasing the recursion depth\n",
    "                recursive_classification(new_groups, final_classification, classification_results, depth + 1, max_depth)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch exceptions and print error messages\n",
    "            print(f\"Error processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise e\n",
    "\n",
    "    return final_classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1866324dbc5e8c92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main function is designed here to directly call the previously defined functions, and then produce the final classification results through the final function (recursive_classification()).\n",
    "\n",
    "Note: Ensure that the groups dictionary, which stores the initial character classification results, contains actual information. This is because the contents of the groups will be overwritten each time the entire process is rerun. Therefore, after completing all API runs, if you need to call it again, consider rerunning the initial API result parsing code to ensure the groups contain actual information."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143aac6b3908d87d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 5\n",
    "\n",
    "# According print the 'groups' dictionary to make sure contain all initial character selection information\n",
    "print(groups)\n",
    "# If don't have any information, please using this function to reload the initial response information\n",
    "# groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "# If also none, please check whether exist the correct initial response information\n",
    "\n",
    "# Assume the variables have been initialized\n",
    "max_depth = 5  # Can be adjusted based on the hierarchical structure of input data and application requirements\n",
    "\n",
    "# Dictionary to store the final classification where each species is classified individually\n",
    "final_classification = {}\n",
    "\n",
    "# Dictionary to store the API classification results for each state\n",
    "classification_results = {}\n",
    "\n",
    "# Print the initial state of groups and dictionaries for debugging purposes\n",
    "print(\"Initial groups:\", groups)\n",
    "print(\"Initial final_classification:\", final_classification)\n",
    "print(\"Initial classification_results:\", classification_results)\n",
    "\n",
    "# Call the recursive_classification function to process the groups and store the results\n",
    "final_classification = recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=max_depth)\n",
    "\n",
    "# Print the final classification results\n",
    "print(\"Final Classification:\")\n",
    "print(json.dumps(final_classification, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the classification results from the API calls\n",
    "print(\"\\nClassification Results:\")\n",
    "print(classification_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f7d855b1816c5e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After obtaining the final classification results, a verification program is designed to help eliminate AI hallucinations and avoid additional errors. Since the information in groups is used in the subsequent verification steps and has been overwritten during the API calls in the classification process, I need to use the previous function that parses groups again to parse the initial response. This ensures that the groups contain the initial character classification results and are in the correct dictionary format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3cfccc8dab5231"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reload the groups information to help with further processing\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(classification_results)\n",
    "print(type(classification_results))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53902333a3f376fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Entering the Verification Section\n",
    "(A flowchart of the verification and correction process can be displayed here.)\n",
    "In this section, the final classification results are parsed, and the selected characters and states for each species are stored in a JSON-formatted list. This allows for easy comparison with the original data in the knowledge graph (JSON format) to check for any discrepancies."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198c492013e90731"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 6\n",
    "\n",
    "# Function to extract paths from the classification tree\n",
    "def extract_paths(node, path=None):\n",
    "    if path is None:\n",
    "        path = {}\n",
    "\n",
    "    if 'Character' in node and 'States' in node:\n",
    "        current_character = node['Character'].replace(\" \", \"\").strip()\n",
    "        for state, value in node['States'].items():\n",
    "            new_path = path.copy()\n",
    "            new_path[current_character] = state\n",
    "            if isinstance(value, dict):\n",
    "                yield from extract_paths(value, new_path)\n",
    "            else:\n",
    "                for species in value:\n",
    "                    yield species, new_path\n",
    "\n",
    "# Process each classification result and extract paths\n",
    "final_results = {}\n",
    "\n",
    "for key, json_str in classification_results.items():\n",
    "    classification_data = json.loads(json_str)\n",
    "    species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "    formatted_results = {}\n",
    "    for species, path in species_paths:\n",
    "        formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "    final_results[key] = formatted_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8907377452a741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, discrepancies from previous results are recorded. If there are differences between the API-generated classification results and the original character and state information, it indicates an error in the API results. These errors are accurately documented, noting which keys in the groups results they originate from. This allows for targeted analysis of these specific keys in subsequent calls to the correction API, thereby saving on API usage costs and the number of calls."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b478fb4d4becf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 7\n",
    "\n",
    "# Function to check if the state matches the correct state\n",
    "def check_state_match(state, correct_state):\n",
    "    if correct_state is None:\n",
    "        return False\n",
    "    if \" and \" in correct_state:\n",
    "        correct_states = correct_state.split(\" and \")\n",
    "        return all(sub_state in correct_states for sub_state in state.split(\" and \"))\n",
    "    return state == correct_state\n",
    "\n",
    "# Validate classification results and log errors\n",
    "def validate_results(final_results, knowledge_graph):\n",
    "    errors = []\n",
    "    for key, results in final_results.items():\n",
    "        for species, data in results.items():\n",
    "            if species in knowledge_graph:\n",
    "                mismatch = False\n",
    "                incorrect_character_states = {}\n",
    "                for character, state in data[\"Characteristics\"].items():\n",
    "                    character = character.replace(\" \", \"\").strip()\n",
    "                    correct_state = knowledge_graph[species][\"Characteristics\"].get(character)\n",
    "                    if correct_state is None or not check_state_match(state, correct_state):\n",
    "                        mismatch = True\n",
    "                        incorrect_character_states[character] = {\"error_state\": state, \"correct_state\": correct_state}\n",
    "                if mismatch:\n",
    "                    errors.append({\n",
    "                        \"species\": species,\n",
    "                        \"key\": key,\n",
    "                        \"error\": \"Mismatch\",\n",
    "                        \"error_result\": incorrect_character_states,\n",
    "                        \"correct_result\": {character: knowledge_graph[species][\"Characteristics\"].get(character) for character in incorrect_character_states}\n",
    "                    })\n",
    "            else:\n",
    "                errors.append({\n",
    "                    \"species\": species,\n",
    "                    \"key\": key,\n",
    "                    \"error\": \"Species not found in knowledge graph\",\n",
    "                    \"error_result\": data[\"Characteristics\"]\n",
    "                })\n",
    "    return errors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b4f5c241b6f9bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, a correction API is created. By retaining the previously recorded error information, we identify which key in the groups contains the error. The error and the correct information are sent back to the API, and only the morphological matrix information related to that key is inputted for the correction API call. However, even with this approach, there might be an issue requiring multiple calls to get the correct result, as LLMs can be persistent and may not always provide the correct feedback based on the error information immediately."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449f9ad5e50987c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 8\n",
    "\n",
    "# Function to get the species list for a specific state from the groups\n",
    "def get_species_list_for_state(groups, key):\n",
    "    species_list = []\n",
    "    for state, species in groups:\n",
    "        if state == key:\n",
    "            species_list = species\n",
    "            break\n",
    "    if not species_list:\n",
    "        print(f\"Key {key} not found in groups\")\n",
    "    else:\n",
    "        print(f\"Processing species list for state '{key}': {species_list}\")\n",
    "    return species_list\n",
    "\n",
    "\n",
    "# Function to correct classification errors using the API\n",
    "def correct_classification(errors, classification_results, knowledge_graph):\n",
    "    for error in errors:\n",
    "        key = error['key']\n",
    "\n",
    "        # Get the species list for the erroneous state\n",
    "        species_list = get_species_list_for_state(groups, key)\n",
    "        if not species_list:\n",
    "            continue\n",
    "\n",
    "        # Create a sub-matrix for the group of species\n",
    "        group_matrix = {s: knowledge_graph[s] for s in species_list}\n",
    "        group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "        # Replace the placeholders in the content templates with actual data\n",
    "        content_error = prompt_messages[\"correct_messages\"][2][\"content_template\"].format(error=error)\n",
    "        content_group_matrix = prompt_messages[\"correct_messages\"][4][\"content_template\"].format(group_matrix_str=group_matrix_str)\n",
    "\n",
    "        # Create messages list\n",
    "        messages_correct = [\n",
    "            prompt_messages[\"correct_messages\"][0],\n",
    "            prompt_messages[\"correct_messages\"][1],\n",
    "            {\"role\": \"user\", \"content\": content_error},\n",
    "            prompt_messages[\"correct_messages\"][3],\n",
    "            {\"role\": \"user\", \"content\": content_group_matrix}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to correct the classification\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_correct,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "            n=1\n",
    "        )\n",
    "        corrected_result = response.choices[0].message.content\n",
    "\n",
    "        # Replace the placeholder in the content template with actual data\n",
    "        content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "            result_secondary=corrected_result\n",
    "        )\n",
    "\n",
    "        # Create messages_JSON list\n",
    "        messages_JSON2 = [\n",
    "            prompt_messages[\"JSON_format_messages\"][0],\n",
    "            prompt_messages[\"JSON_format_messages\"][1],\n",
    "            prompt_messages[\"JSON_format_messages\"][2],\n",
    "            {\"role\": \"user\", \"content\": content_with_data}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to format the response as JSON\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_JSON2,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1500,\n",
    "            n=1\n",
    "        )\n",
    "        json_result = response.choices[0].message.content\n",
    "        json_cleaned_result = extract_json_string(json_result)\n",
    "        print(json_cleaned_result)\n",
    "        classification_results[key] = json_cleaned_result\n",
    "        return classification_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8587edde23dcdb2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, a while loop is set up for the correction process. Due to AI hallucinations related to the LLM API, the correction API may not always provide the correct answer in a single attempt, even with erroneous API results. This necessitates a continual process of correction and verification. After each correction, the previous function steps are called to check if the characters and states of the species match the original dataset. If discrepancies are found, the error is recorded, and the correction API is called again. This process continues until the final results match the original character and state information, resulting in a complete and correct outcome."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "111c0434c4cb0f84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 9\n",
    "\n",
    "# Validate the initial classification results and log any errors\n",
    "errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Purpose: Enter a loop until all errors have been fixed.\n",
    "# Function: Executes the code inside the loop when the errors list is not empty.\n",
    "while errors:\n",
    "    # Fix current categorization errors using the API\n",
    "    classification_results = correct_classification(errors, classification_results, knowledge_graph)\n",
    "\n",
    "    # Reset the final_results dictionary to store the corrected categorization results\n",
    "    final_results = {}\n",
    "\n",
    "    # Iterate over the corrected classification results and extract species classification paths\n",
    "    for key, json_str in classification_results.items():\n",
    "        classification_data = json.loads(json_str)\n",
    "        species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "        # Format the extracted classification paths and store them in the formatted_results dictionary\n",
    "        formatted_results = {}\n",
    "        for species, path in species_paths:\n",
    "            formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "        # Add the formatted classification results to final_results\n",
    "        final_results[key] = formatted_results\n",
    "\n",
    "    # Re-validate the corrected classification results and log any remaining errors\n",
    "    errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Save the final classification results to a JSON file\n",
    "with open('final_classification.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(\"Final classification results have been saved to 'final_classification.json'.\")\n",
    "print(json.dumps(final_results, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "895880990113e7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the classification key actually contains the real information for the corresponding characters and states, meaning the descriptions for each character and state, we initially parsed the Nexus file for character labels and state labels to obtain the description information for each character and state. Therefore, we need to perform character mapping to include these descriptions in the final results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3315aded8b2352c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 10\n",
    "\n",
    "# Convert classification_results JSON strings to dictionaries\n",
    "classification_result = {key: json.loads(value) for key, value in classification_results.items()}\n",
    "\n",
    "\n",
    "# Recursive function to convert structure to the desired format\n",
    "def convert_structure(node):\n",
    "    if \"Character\" in node and \"States\" in node:\n",
    "        character = node[\"Character\"]\n",
    "        states = node[\"States\"]\n",
    "        converted = {f\"Character {character.replace('Character', '')}\": {}}\n",
    "        for state, sub_node in states.items():\n",
    "            state_key = f\"State {state}\"\n",
    "            if isinstance(sub_node, list):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = sub_node[0] if len(sub_node) == 1 else sub_node\n",
    "            elif isinstance(sub_node, dict):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = convert_structure(sub_node)\n",
    "        return converted\n",
    "    return node\n",
    "\n",
    "\n",
    "# Process classification results to the desired format\n",
    "converted_result = {}\n",
    "for key, value in classification_result.items():\n",
    "    converted_result[f\"Character {key}\"] = convert_structure(value)\n",
    "\n",
    "\n",
    "# Combine initial classification with other results\n",
    "def combine_results(initial, secondary, state_key):\n",
    "    if not secondary:\n",
    "        return\n",
    "\n",
    "    initial_states = initial[\"States\"].get(state_key)\n",
    "    if initial_states is None:\n",
    "        initial[\"States\"][state_key] = secondary\n",
    "        return\n",
    "\n",
    "    if isinstance(initial_states, list):\n",
    "        if isinstance(secondary, list):\n",
    "            initial[\"States\"][state_key] = list(set(initial_states + secondary))  # Merge two lists and remove duplicates\n",
    "        else:\n",
    "            initial[\"States\"][state_key] = secondary\n",
    "    elif isinstance(initial_states, dict):\n",
    "        if isinstance(secondary, dict):\n",
    "            for key, value in secondary[\"States\"].items():\n",
    "                if key not in initial_states:\n",
    "                    initial_states[key] = value\n",
    "                else:\n",
    "                    combine_results(initial_states, value, key)\n",
    "        else:\n",
    "            raise ValueError(f\"Conflicting types for key {state_key}: {type(initial_states)} vs {type(secondary)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for initial states: {type(initial_states)}\")\n",
    "\n",
    "\n",
    "# Dynamically combine all secondary classification results\n",
    "for state_key, secondary in classification_result.items():\n",
    "    combine_results(parsed_initial_classification, secondary, state_key)\n",
    "\n",
    "# Convert the merged results to the desired format\n",
    "converted_initial_classification = convert_structure(parsed_initial_classification)\n",
    "\n",
    "# Recursive function to replace indices with descriptions in the classification key\n",
    "def replace_indices_with_descriptions_in_key(key, character_info, parent_char_index=None):\n",
    "    updated_key = {}\n",
    "    for char_state, subtree in key.items():\n",
    "        if char_state.startswith(\"Character\"):\n",
    "            parts = char_state.split()\n",
    "            if len(parts) > 1:\n",
    "                char_index = parts[1]\n",
    "                if char_index in character_info:\n",
    "                    char_description = f\"Character {char_index}: {character_info[char_index]['description']}\"\n",
    "                    if isinstance(subtree, dict):\n",
    "                        updated_subtree = replace_indices_with_descriptions_in_key(subtree, character_info, char_index)\n",
    "                        updated_key[char_description] = updated_subtree\n",
    "                    else:\n",
    "                        updated_key[char_description] = subtree\n",
    "                else:\n",
    "                    updated_key[char_state] = subtree\n",
    "            else:\n",
    "                updated_key[char_state] = subtree\n",
    "        elif char_state.startswith(\"State\") and parent_char_index:\n",
    "            states = char_state.split()[1:]\n",
    "            state_descriptions = []\n",
    "            for state in states:\n",
    "                individual_states = state.split(\"and\")\n",
    "                descriptions = [character_info[parent_char_index][\"states\"].get(s.strip(), \"\") for s in individual_states]\n",
    "                state_descriptions.append(\" and \".join(filter(None, descriptions)))\n",
    "            state_key = f\"State {' '.join(states)}: {' / '.join(state_descriptions)}\"\n",
    "            if isinstance(subtree, dict):\n",
    "                updated_key[state_key] = replace_indices_with_descriptions_in_key(subtree, character_info, parent_char_index)\n",
    "            else:\n",
    "                updated_key[state_key] = subtree\n",
    "        else:\n",
    "            updated_key[char_state] = subtree\n",
    "    return updated_key\n",
    "\n",
    "\n",
    "# Replace feature and state descriptions\n",
    "updated_classification_key = replace_indices_with_descriptions_in_key(converted_initial_classification, character_info)\n",
    "\n",
    "# Print the updated classification key\n",
    "print(\"Updated Classification Key:\")\n",
    "print(json.dumps(updated_classification_key, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d65fd4d4cf7fda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally, since the final output format is JSON, which uses nested structures to display the generated classification key, the results may not be very intuitive. Therefore, to achieve a more visually appealing and concise classification key, this section can be used to optimize the final output format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3433a36d44353582"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 11\n",
    "\n",
    "# Initialize step counter\n",
    "step_counter = 1\n",
    "steps = []\n",
    "outer_steps = []\n",
    "\n",
    "\n",
    "# Recursive function to generate classification key\n",
    "def generate_classification_key(data, current_step, parent_step=None, outer_step=False):\n",
    "    global step_counter\n",
    "    if isinstance(data, dict):\n",
    "        for character, states in data.items():\n",
    "            # Ensure step_prefix is correct\n",
    "            if parent_step is None:\n",
    "                step_prefix = str(current_step)\n",
    "            else:\n",
    "                step_prefix = f\"{current_step}({parent_step})\"\n",
    "\n",
    "            step_description = f\"{step_prefix}. {character.split(': ')[1]}\"  # Only use description\n",
    "            if outer_step:\n",
    "                outer_steps.append(step_description)  # Record outermost steps\n",
    "            else:\n",
    "                steps.append(step_description)  # Record current step\n",
    "            state_steps = []\n",
    "            step_map = {}\n",
    "            for state, next_level in states.items():\n",
    "                if isinstance(next_level, dict):\n",
    "                    step_counter += 1\n",
    "                    next_step_prefix = str(step_counter) if parent_step is None else f\"{step_counter}\"\n",
    "                    state_steps.append(f\"    - {state.split(': ')[1]} ........ {next_step_prefix}\")  # Only use description\n",
    "                    step_map[step_counter] = (next_level, current_step)\n",
    "                else:\n",
    "                    state_steps.append(f\"    - {state.split(': ')[1]} ........ {next_level}\")  # Only use description\n",
    "            steps.extend(state_steps)\n",
    "            for step, (next_level, current_step) in step_map.items():\n",
    "                generate_classification_key(next_level, step, current_step)\n",
    "            current_step += 1  # Ensure the outermost steps are in order\n",
    "    else:\n",
    "        # If data is not a dictionary, do not recurse\n",
    "        return\n",
    "\n",
    "\n",
    "# Generate classification key\n",
    "generate_classification_key(updated_classification_key, 1, outer_step=True)\n",
    "\n",
    "# Sort the outermost steps\n",
    "sorted_outer_steps = sorted(outer_steps, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "# Combine sorted outermost steps with other steps\n",
    "sorted_steps = sorted_outer_steps + [step for step in steps if not any(step.startswith(f\"{i}.\") for i in range(1, len(outer_steps) + 1))]\n",
    "\n",
    "# Format output\n",
    "classification_key = \"\\n\".join(sorted_steps)\n",
    "print(classification_key)\n",
    "\n",
    "# Write results to file\n",
    "with open(\"classification_key.txt\", \"w\") as f:\n",
    "    f.write(classification_key)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ce671d86564c87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
